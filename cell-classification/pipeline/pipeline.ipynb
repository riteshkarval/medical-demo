{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install pipelines SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please wait till this cell completes and then run next cells. This just need to be run once per active kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: RELEASE_VERSION=0.1.18\n",
      "Collecting https://storage.googleapis.com/ml-pipeline/release/0.1.18/kfp.tar.gz\n",
      "  Using cached https://storage.googleapis.com/ml-pipeline/release/0.1.18/kfp.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (1.13.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (5.3)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (1.24.1)\n",
      "Requirement already satisfied, skipping upgrade: kubernetes<=9.0.0,>=8.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (9.0.0)\n",
      "Requirement already satisfied, skipping upgrade: PyJWT>=1.6.4 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: cryptography>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (1.10.0)\n",
      "Requirement already satisfied, skipping upgrade: requests_toolbelt>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from kfp==0.1.18) (0.9.1)\n",
      "Requirement already satisfied, skipping upgrade: google-resumable-media<0.6dev,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.1.18) (0.5.0)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp==0.1.18) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=9.0.0,>=8.0.0->kfp==0.1.18) (45.0.0.post20200113)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.7/site-packages (from kubernetes<=9.0.0,>=8.0.0->kfp==0.1.18) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<=9.0.0,>=8.0.0->kfp==0.1.18) (0.57.0)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<=9.0.0,>=8.0.0->kfp==0.1.18) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: cffi!=1.11.3,>=1.8 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.4.2->kfp==0.1.18) (1.13.2)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.1.18) (4.0.0)\n",
      "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.1.18) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp==0.1.18) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core<2.0.0dev,>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.1.0->google-cloud-storage>=1.13.0->kfp==0.1.18) (1.16.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=9.0.0,>=8.0.0->kfp==0.1.18) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<=9.0.0,>=8.0.0->kfp==0.1.18) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<=9.0.0,>=8.0.0->kfp==0.1.18) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.4.2->kfp==0.1.18) (2.19)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth>=1.6.1->kfp==0.1.18) (0.4.8)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.1.0->google-cloud-storage>=1.13.0->kfp==0.1.18) (3.11.2)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.1.0->google-cloud-storage>=1.13.0->kfp==0.1.18) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.1.0->google-cloud-storage>=1.13.0->kfp==0.1.18) (1.6.0)\n",
      "Building wheels for collected packages: kfp\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-0.1.18-cp37-none-any.whl size=242155 sha256=2fcd9a3e70b1170de8c6836712ba5fd0a2a7d2b2e91f2c452e6ff0c3e9b33ce2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-6shdpr0f/wheels/a8/be/46/964c241809fd50c2a15d63d0d61e4c48596c075dce5a83265c\n",
      "Successfully built kfp\n",
      "Installing collected packages: kfp\n",
      "  Found existing installation: kfp 0.1.18\n",
      "    Uninstalling kfp-0.1.18:\n",
      "      Successfully uninstalled kfp-0.1.18\n",
      "Successfully installed kfp-0.1.18\n"
     ]
    }
   ],
   "source": [
    "%env RELEASE_VERSION=0.1.18\n",
    "!pip install https://storage.googleapis.com/ml-pipeline/release/${RELEASE_VERSION}/kfp.tar.gz --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import kfp pkgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.compiler as compiler\n",
    "from kubernetes import client as k8s_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List existing pipeline experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiments': [{'created_at': datetime.datetime(2020, 1, 21, 12, 50, 51, tzinfo=tzlocal()),\n",
       "                  'description': 'All runs created without specifying an '\n",
       "                                 'experiment will be grouped here.',\n",
       "                  'id': '5bda554f-253b-4127-b57b-85e1564ccc28',\n",
       "                  'name': 'Default'},\n",
       "                 {'created_at': datetime.datetime(2020, 1, 21, 13, 19, 7, tzinfo=tzlocal()),\n",
       "                  'description': None,\n",
       "                  'id': '7101279c-7494-4bde-9555-83f674e4a711',\n",
       "                  'name': 'Dkube - Regression pl'}],\n",
       " 'next_page_token': None,\n",
       " 'total_size': 2}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = kfp.Client()\n",
    "client.list_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Dkube Nucleoplasm_Cytosol experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/7101279c-7494-4bde-9555-83f674e4a711\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Nucleoplasm_Cytosol_experiment = client.create_experiment(name='Dkube - Nucleoplasm_Cytosol pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define e2e Nucleoplasm_Cytosol Pipeline with Dkube components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "from kfp import components\n",
    "from kubernetes import client as k8s_client\n",
    "\n",
    "import os\n",
    "import json\n",
    "from random import randint\n",
    "\n",
    "dkube_preprocess_op         = components.load_component_from_file(\"../components/preprocess/component.yaml\")\n",
    "dkube_training_op           = components.load_component_from_file(\"../components/training/component.yaml\")\n",
    "dkube_serving_op            = components.load_component_from_file(\"../components/serving/component.yaml\")\n",
    "dkube_viewer_op             = components.load_component_from_file('../components/viewer/component.yaml')\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='dkube-regression-pl',\n",
    "    description='sample regression pipeline with dkube components'\n",
    ")\n",
    "\n",
    "def d3pipeline(\n",
    "    #cellular preprocess\n",
    "    cellular_preprocess_script=\"python cell-classification/preprocessing/merge.py\",\n",
    "    cellular_preprocess_datasets=json.dumps([\"cellular\"]),\n",
    "    cellular_preprocess_input_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    cellular_preprocess_outputs=json.dumps([\"cellular-preprocessed\"]),\n",
    "    cellular_preprocess_output_mounts=json.dumps([\"/opt/dkube/output\"]),\n",
    "    \n",
    "    #cellular split\n",
    "    cellular_split_script=\"python cell-classification/split/annot_split.py\",\n",
    "    cellular_split_datasets=json.dumps([\"cellular-preprocessed\"]),\n",
    "    cellular_split_input_mounts=json.dumps([\"/opt/dkube/input\"]),\n",
    "    cellular_split_outputs=json.dumps([\"cellular-train\", \"cellular-test\"]),\n",
    "    cellular_split_output_mounts=json.dumps([\"/opt/dkube/output/train\", \"/opt/dkube/output/test\"]),\n",
    "    \n",
    "    #Training\n",
    "    #In notebook DKUBE_USER_ACCESS_TOKEN is automatically picked up from env variable\n",
    "    auth_token  = os.getenv(\"DKUBE_USER_ACCESS_TOKEN\"),\n",
    "    #By default tf v1.14 image is used here, v1.13 or v1.14 can be used. \n",
    "    #Or any other custom image name can be supplied.\n",
    "    #For custom private images, please input username/password\n",
    "    training_container=json.dumps({'image':'docker.io/ocdr/d3-datascience-tf-cpu:v1.14', 'username':'', 'password': ''}),\n",
    "    #Name of the workspace in dkube. Update accordingly if different name is used while creating a workspace in dkube.\n",
    "    training_program=\"faster-rcnn\",\n",
    "    #Script to run inside the training container    \n",
    "    training_script=\"python cell-classification/model/train_frcnn.py -o simple -p /opt/dkube/input/annot.txt --hf --vf --rot --num_epochs 1\",\n",
    "    #Input datasets for training. Update accordingly if different name is used while creating dataset in dkube.    \n",
    "    training_datasets=json.dumps([\"train\"]),\n",
    "    training_input_dataset_mounts=json.dumps([\"/opt/dkube/input/\"]),\n",
    "    training_outputs=json.dumps([\"faster-rcnn\"]),\n",
    "    training_output_mounts=json.dumps([\"/opt/dkube/output\"]),\n",
    "    #Request gpus as needed. Val 0 means no gpu, then training_container=docker.io/ocdr/dkube-datascience-tf-cpu:v1.12    \n",
    "    training_gpus=0,\n",
    "    #Any envs to be passed to the training program    \n",
    "    training_envs=json.dumps([{\"steps\": 100}]),\n",
    "    \n",
    "    #Evaluation\n",
    "    evaluation_script=\"python cell-classification/model/evaluate.py --path /opt/dkube/input/annot.txt\",\n",
    "    evaluation_datasets=json.dumps([\"test\"]),\n",
    "    evaluation_input_dataset_mounts=json.dumps([\"/opt/dkube/inputs/\"]),\n",
    "    evaluation_models=json.dumps([\"faster-rcnn\"]),\n",
    "    evaluation_input_model_mounts=json.dumps([\"/opt/dkube/model\"]),\n",
    "    \n",
    "    #Serving\n",
    "    #Device to be used for serving - dkube mnist example trained on gpu needs gpu for serving else set this param to 'cpu'\n",
    "    serving_device='cpu',\n",
    "    serving_container=json.dumps({'image':'docker.io/ocdr/new-preprocess:satish', 'username':'', 'password': ''})):\n",
    "    \n",
    "    cellular_preprocess  = dkube_preprocess_op(auth_token, training_container,\n",
    "                                      program=training_program, run_script=cellular_preprocess_script,\n",
    "                                      datasets=cellular_preprocess_datasets, outputs=cellular_preprocess_outputs,\n",
    "                                      input_dataset_mounts=cellular_preprocess_input_mounts, output_mounts=cellular_preprocess_output_mounts)\n",
    "\n",
    "\n",
    "    cellular_split  = dkube_preprocess_op(auth_token, training_container,\n",
    "                                      program=training_program, run_script=cellular_split_script,\n",
    "                                      datasets=cellular_split_datasets, outputs=cellular_split_outputs,\n",
    "                                      input_dataset_mounts=cellular_split_input_mounts,\n",
    "                                      output_mounts=cellular_split_output_mounts).after(cellular_preprocess)\n",
    "                                      \n",
    "                                    \n",
    "    train       = dkube_training_op(auth_token, training_container,\n",
    "                                    program=training_program, run_script=training_script,\n",
    "                                    datasets=training_datasets, outputs=training_outputs,\n",
    "                                    input_dataset_mounts=training_input_dataset_mounts,\n",
    "                                    output_mounts=training_output_mounts,\n",
    "                                    ngpus=training_gpus,\n",
    "                                    envs=training_envs).after(cellular_split)\n",
    "                                    \n",
    "    evaluate    = dkube_training_op(auth_token, training_container,\n",
    "                                    program=training_program, run_script=evaluation_script,\n",
    "                                    datasets=evaluation_datasets,\n",
    "                                    input_dataset_mounts=evaluation_input_dataset_mounts,\n",
    "                                    models=evaluation_models,\n",
    "                                    input_model_mounts=evaluation_input_model_mounts,\n",
    "                                    ngpus=training_gpus,\n",
    "                                    envs=training_envs).after(train)\n",
    "    # serving     = dkube_serving_op(auth_token, train.outputs['artifact'], device=serving_device, serving_container=serving_container).after(evaluate)\n",
    "    #inference   = dkube_viewer_op(auth_token, serving.outputs['servingurl'],\n",
    "    #                              'digits', viewtype='inference').after(serving)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and generate tar ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(d3pipeline, 'dkube_Nucleoplasm_Cytosol_pl.tar.gz')\n",
    "# Upload this generated tarball into the Pipelines UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Run pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Click the dkube-training stage to see the enhanced Dkube Datascience dashboard, metrics and graphs. Click the dkube-viewer stage for the simple UI to test the model predecitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/a46b2114-ccdb-4e63-9af6-a9c0c66f740b\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = client.run_pipeline(regression_experiment.id, 'Nucleoplasm_Cytosol_pipeline', 'dkube_Nucleoplasm_Cytosol_pl.tar.gz', params={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
