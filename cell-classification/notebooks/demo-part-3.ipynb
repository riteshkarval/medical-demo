{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import pickle\n",
    "from optparse import OptionParser\n",
    "import time\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from keras_frcnn import config\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from keras_frcnn import data_generators\n",
    "from keras_frcnn import losses as losses\n",
    "import keras_frcnn.roi_helpers as roi_helpers\n",
    "from keras_frcnn.simple_parser import get_data\n",
    "from keras.utils import generic_utils\n",
    "\n",
    "def format_img_size(img, C):\n",
    "    \"\"\" formats the image size based on config \"\"\"\n",
    "    img_min_side = float(C.im_size)\n",
    "    (height,width,_) = img.shape\n",
    "        \n",
    "    if width <= height:\n",
    "        ratio = img_min_side/width\n",
    "        new_height = int(ratio * height)\n",
    "        new_width = int(img_min_side)\n",
    "    else:\n",
    "        ratio = img_min_side/height\n",
    "        new_width = int(ratio * width)\n",
    "        new_height = int(img_min_side)\n",
    "    img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
    "    return img, ratio    \n",
    "\n",
    "def format_img_channels(img, C):\n",
    "    \"\"\" formats the image channels based on config \"\"\"\n",
    "    img = img[:, :, (2, 1, 0)]\n",
    "    img = img.astype(np.float32)\n",
    "    img[:, :, 0] -= C.img_channel_mean[0]\n",
    "    img[:, :, 1] -= C.img_channel_mean[1]\n",
    "    img[:, :, 2] -= C.img_channel_mean[2]\n",
    "    img /= C.img_scaling_factor\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "def format_img(img, C):\n",
    "    \"\"\" formats an image for model prediction based on config \"\"\"\n",
    "    img, ratio = format_img_size(img, C)\n",
    "    img = format_img_channels(img, C)\n",
    "    return img, ratio\n",
    "\n",
    "# Method to transform the coordinates of the bounding box to its original size\n",
    "def get_real_coordinates(ratio, x1, y1, x2, y2):\n",
    "\n",
    "    real_x1 = int(round(x1 // ratio))\n",
    "    real_y1 = int(round(y1 // ratio))\n",
    "    real_x2 = int(round(x2 // ratio))\n",
    "    real_y2 = int(round(y2 // ratio))\n",
    "\n",
    "    return (real_x1, real_y1, real_x2 ,real_y2)\n",
    "\n",
    "sys.setrecursionlimit(40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = '/home/dkube/split/test/annot.txt'\n",
    "num_rois = 32\n",
    "parser = \"simple\"\n",
    "network = 'resnet50'\n",
    "horizontal_flips = False\n",
    "vertical_flips = False\n",
    "rot_90 = False\n",
    "num_epochs = 5\n",
    "config_filename = \"config.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing annotation files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0224 01:20:30.318214 140401503553344 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num test samples 290\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0224 01:20:35.470502 140401503553344 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0224 01:20:35.505142 140401503553344 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0224 01:20:36.055023 140401503553344 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/converters/directives.py:117: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from /home/dkube/model/weights.h5\n",
      "Evaluating 290 samples\n"
     ]
    }
   ],
   "source": [
    "# data_path = '/opt/dkube/input/'/\n",
    "\n",
    "model_path ='/home/dkube/model/'\n",
    "\n",
    "config_output_filename = model_path +  config_filename\n",
    "\n",
    "with open(config_output_filename, 'rb') as f_in:\n",
    "    C = pickle.load(f_in)\n",
    "\n",
    "if C.network == 'resnet50':\n",
    "    import keras_frcnn.resnet as nn\n",
    "elif C.network == 'vgg':\n",
    "    import keras_frcnn.vgg as nn\n",
    "\n",
    "# turn off any data augmentation at test time\n",
    "C.use_horizontal_flips = False\n",
    "C.use_vertical_flips = False\n",
    "C.rot_90 = False\n",
    "\n",
    "class_mapping = C.class_mapping\n",
    "    \n",
    "    \n",
    "all_imgs, classes_count, class_mapping = get_data( test_path)\n",
    "\n",
    "if 'bg' not in classes_count:\n",
    "    classes_count['bg'] = 0\n",
    "    class_mapping['bg'] = len(class_mapping)\n",
    "    \n",
    "test_imgs = [s for s in all_imgs if s['imageset'] == 'trainval' or s['imageset'] == 'test']\n",
    "\n",
    "print('Num test samples {}'.format(len(test_imgs)))\n",
    "\n",
    "data_gen_test = data_generators.get_anchor_gt(test_imgs, classes_count, C, nn.get_img_output_length, mode='val')\n",
    "\n",
    "C.num_rois = int( num_rois)\n",
    "\n",
    "if C.network == 'resnet50':\n",
    "    num_features = 1024\n",
    "elif C.network == 'vgg':\n",
    "    num_features = 512\n",
    "\n",
    "\n",
    "input_shape_img = (600,600, 3)\n",
    "\n",
    "img_input = Input(shape=input_shape_img)\n",
    "roi_input = Input(shape=(None, 4))\n",
    "# feature_map_input = Input(shape=input_shape_features)\n",
    "\n",
    "shared_layers = nn.nn_base(img_input, trainable=True)\n",
    "\n",
    "# define the RPN, built on the base layers\n",
    "num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\n",
    "rpn = nn.rpn(shared_layers, num_anchors)\n",
    "\n",
    "classifier = nn.classifier(shared_layers, roi_input, C.num_rois, nb_classes=len(classes_count), trainable=True)\n",
    "\n",
    "model_rpn = Model(img_input, rpn[:2])\n",
    "model_classifier = Model([img_input, roi_input], classifier)\n",
    "\n",
    "C.model_path = model_path + 'weights.h5'\n",
    "\n",
    "\n",
    "print('Loading weights from {}'.format(C.model_path))\n",
    "model_rpn.load_weights(C.model_path, by_name=True)\n",
    "model_classifier.load_weights(C.model_path, by_name=True)\n",
    "\n",
    "optimizer = Adam(lr=1e-5)\n",
    "optimizer_classifier = Adam(lr=1e-5)\n",
    "model_rpn.compile(optimizer=optimizer, loss=[losses.rpn_loss_cls(num_anchors), losses.rpn_loss_regr(num_anchors)])\n",
    "model_classifier.compile(optimizer=optimizer_classifier, loss=[losses.class_loss_cls, losses.class_loss_regr(len(classes_count)-1)], metrics={'dense_class_{}'.format(len(classes_count)): 'accuracy'})\n",
    "\n",
    "################# Evaluating #################################\n",
    "start_time = time.time()\n",
    "losses = np.zeros((len(test_imgs), 5))\n",
    "rpn_accuracy_rpn_monitor = []\n",
    "rpn_accuracy_for_epoch = []\n",
    "iter_num = 0\n",
    "metric_names = ['loss_rpn_cls','loss_rpn_regr', 'loss_class_cls',\n",
    "                'loss_class_regr', 'class_acc', 'mean_overlapping_bboxes']\n",
    "progbar = generic_utils.Progbar(len(test_imgs))\n",
    "print('Evaluating {} samples'.format(len(test_imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/sample - loss: 2.4272 - rpn_out_class_loss: 1.5758 - rpn_out_regress_loss: 0.8514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0224 01:21:10.505896 140401503553344 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 683ms/sample - loss: 3.3040 - rpn_out_class_loss: 3.2465 - rpn_out_regress_loss: 0.0575gr: 0.4317\n",
      "1/1 [==============================] - 1s 955ms/sample - loss: 3.9274 - rpn_out_class_loss: 3.6155 - rpn_out_regress_loss: 0.3119gr: 0.3871\n",
      "1/1 [==============================] - 1s 933ms/sample - loss: 0.8570 - rpn_out_class_loss: 0.0836 - rpn_out_regress_loss: 0.7735gr: 0.4089\n",
      "1/1 [==============================] - 1s 1s/sample - loss: 3.4420 - rpn_out_class_loss: 3.2900 - rpn_out_regress_loss: 0.1521_regr: 0.3067\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    X, Y, img_data = next(data_gen_test)\n",
    "    loss_rpn = model_rpn.evaluate(X, Y)\n",
    "    P_rpn = model_rpn.predict_on_batch(X)\n",
    "\n",
    "    R = roi_helpers.rpn_to_roi(P_rpn[0], P_rpn[1], C, use_regr=True, overlap_thresh=0.7, max_boxes=300)\n",
    "    # note: calc_iou converts from (x1,y1,x2,y2) to (x,y,w,h) format\n",
    "    X2, Y1, Y2, IouS = roi_helpers.calc_iou(R, img_data, C, class_mapping)\n",
    "\n",
    "    if X2 is None:\n",
    "        rpn_accuracy_rpn_monitor.append(0)\n",
    "        rpn_accuracy_for_epoch.append(0)\n",
    "        continue\n",
    "\n",
    "    neg_samples = np.where(Y1[0, :, -1] == 1)\n",
    "    pos_samples = np.where(Y1[0, :, -1] == 0)\n",
    "\n",
    "    if len(neg_samples) > 0:\n",
    "        neg_samples = neg_samples[0]\n",
    "    else:\n",
    "        neg_samples = []\n",
    "\n",
    "    if len(pos_samples) > 0:\n",
    "        pos_samples = pos_samples[0]\n",
    "    else:\n",
    "        pos_samples = []\n",
    "    \n",
    "    rpn_accuracy_rpn_monitor.append(len(pos_samples))\n",
    "    rpn_accuracy_for_epoch.append((len(pos_samples)))\n",
    "\n",
    "    if C.num_rois > 1:\n",
    "        if len(pos_samples) < C.num_rois//2:\n",
    "            selected_pos_samples = pos_samples.tolist()\n",
    "        else:\n",
    "            selected_pos_samples = np.random.choice(pos_samples, C.num_rois//2, replace=False).tolist()\n",
    "        try:\n",
    "            selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=False).tolist()\n",
    "        except:\n",
    "            selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=True).tolist()\n",
    "\n",
    "        sel_samples = selected_pos_samples + selected_neg_samples\n",
    "    else:\n",
    "        # in the extreme case where num_rois = 1, we pick a random pos or neg sample\n",
    "        selected_pos_samples = pos_samples.tolist()\n",
    "        selected_neg_samples = neg_samples.tolist()\n",
    "        if np.random.randint(0, 2):\n",
    "            sel_samples = random.choice(neg_samples)\n",
    "        else:\n",
    "            sel_samples = random.choice(pos_samples)\n",
    "\n",
    "    loss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]], [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])\n",
    "\n",
    "    losses[iter_num, 0] = loss_rpn[1]\n",
    "    losses[iter_num, 1] = loss_rpn[2]\n",
    "\n",
    "    losses[iter_num, 2] = loss_class[1]\n",
    "    losses[iter_num, 3] = loss_class[2]\n",
    "    losses[iter_num, 4] = loss_class[3]\n",
    "\n",
    "    progbar.update(iter_num+1, [('rpn_cls', losses[iter_num, 0]), ('rpn_regr', losses[iter_num, 1]),\n",
    "                                ('detector_cls', losses[iter_num, 2]), ('detector_regr', losses[iter_num, 3])])\n",
    "\n",
    "    iter_num += 1\n",
    "    \n",
    "    if iter_num == len(test_imgs):\n",
    "        loss_rpn_cls = np.mean(losses[:, 0])\n",
    "        loss_rpn_regr = np.mean(losses[:, 1])\n",
    "        loss_class_cls = np.mean(losses[:, 2])\n",
    "        loss_class_regr = np.mean(losses[:, 3])\n",
    "        class_acc = np.mean(losses[:, 4])\n",
    "        mean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)\n",
    "        \n",
    "        rpn_accuracy_for_epoch = []\n",
    "        \n",
    "        if C.verbose:\n",
    "            print('Mean number of bounding boxes from RPN overlapping ground truth boxes: {}'.format(mean_overlapping_bboxes))\n",
    "            print('Classifier accuracy for bounding boxes from RPN: {}'.format(class_acc))\n",
    "            print('Loss RPN classifier: {}'.format(loss_rpn_cls))\n",
    "            print('Loss RPN regression: {}'.format(loss_rpn_regr))\n",
    "            print('Loss Detector classifier: {}'.format(loss_class_cls))\n",
    "            print('Loss Detector regression: {}'.format(loss_class_regr))\n",
    "            print('Elapsed time: {}'.format(time.time() - start_time))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
