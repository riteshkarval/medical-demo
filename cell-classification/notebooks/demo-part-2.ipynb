{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# python train_frcnn.py -o simple -p /home/dkube/work/workspace/split/train/annot.txt --hf --vf --rot --num_epochs 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0218 09:37:44.373408 139714767529792 deprecation_wrapper.py:119] From /opt/dkube/dkube.py:8: The name tf.train.LoggingTensorHook is deprecated. Please use tf.estimator.LoggingTensorHook instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pprint\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import pickle\n",
    "import os, json\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from keras_frcnn import config, data_generators\n",
    "from keras_frcnn import losses as losses\n",
    "import keras_frcnn.roi_helpers as roi_helpers\n",
    "from keras.utils import generic_utils, get_file\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from dkube import dkubeLoggerHook as logger_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_log(callback, names, logs, batch_no):\n",
    "    for name, value in zip(names, logs):\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.simple_value = value\n",
    "        summary_value.tag = name\n",
    "        callback.writer.add_summary(summary, batch_no)\n",
    "        callback.writer.flush()\n",
    "\n",
    "train_path = '/home/dkube/split/train/annot.txt'\n",
    "num_rois = 32\n",
    "parser = \"simple\"\n",
    "network = 'resnet50'\n",
    "horizontal_flips = False\n",
    "vertical_flips = False\n",
    "rot_90 = False\n",
    "num_epochs = 5\n",
    "config_filename = \"config.pickle\"\n",
    "output_weight_path = './model_frcnn.hdf5'\n",
    "input_weight_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing annotation files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0218 09:49:41.616479 139714767529792 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images per class:\n",
      "{'Cytosol': 11564, 'Nucleoplasm': 21234, 'bg': 0}\n",
      "Num classes (including bg) = 3\n",
      "Config has been written to config.pickle, and can be loaded when testing to ensure correct results\n",
      "Num train samples 2291\n",
      "Num val samples 449\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0218 09:49:45.821241 139714767529792 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0218 09:49:45.855235 139714767529792 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0218 09:49:46.005438 139714767529792 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/converters/directives.py:117: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
      "Starting training\n"
     ]
    }
   ],
   "source": [
    "modeldir = '/home/dkube/work/workspace/model/'\n",
    "metric_path = '/home/dkube/work/workspace/metric/'\n",
    "all_export_path = modeldir + 'all_model/'\n",
    "rpn_export_path = modeldir + 'rpn_model/'\n",
    "clf_export_path = modeldir + 'clf_model/'\n",
    "log_path = modeldir + 'logs/'\n",
    "\n",
    "if not  train_path:   # if filename is not given\n",
    "    parser.error('Error: path to training data must be specified. Pass --path to command line')\n",
    "\n",
    "if  parser == 'pascal_voc':\n",
    "    from keras_frcnn.pascal_voc_parser import get_data\n",
    "elif  parser == 'simple':\n",
    "    from keras_frcnn.simple_parser import get_data\n",
    "else:\n",
    "    raise ValueError(\"Command line option parser must be one of 'pascal_voc' or 'simple'\")\n",
    "\n",
    "# pass the settings from the command line, and persist them in the config object\n",
    "C = config.Config()\n",
    "\n",
    "C.use_horizontal_flips = bool( horizontal_flips)\n",
    "C.use_vertical_flips = bool( vertical_flips)\n",
    "C.rot_90 = bool( rot_90)\n",
    "\n",
    "C.model_path =  output_weight_path\n",
    "C.num_rois = int( num_rois)\n",
    "\n",
    "if  network == 'vgg':\n",
    "    C.network = 'vgg'\n",
    "    from keras_frcnn import vgg as nn\n",
    "elif  network == 'resnet50':\n",
    "    from keras_frcnn import resnet as nn\n",
    "    C.network = 'resnet50'\n",
    "else:\n",
    "    print('Not a valid model')\n",
    "    raise ValueError\n",
    "\n",
    "\n",
    "\n",
    "if not tf.io.gfile.exists(modeldir):\n",
    "    tf.io.gfile.makedirs(modeldir)\n",
    "\n",
    "# check if weight path was passed via command line\n",
    "if  input_weight_path:\n",
    "    C.base_net_weights =  input_weight_path\n",
    "else:\n",
    "    # set the path to weights based on backend and model\n",
    "    C.base_net_weights = nn.get_weight_path()\n",
    "\n",
    "all_imgs, classes_count, class_mapping = get_data( train_path)\n",
    "\n",
    "if 'bg' not in classes_count:\n",
    "    classes_count['bg'] = 0\n",
    "    class_mapping['bg'] = len(class_mapping)\n",
    "\n",
    "C.class_mapping = class_mapping\n",
    "\n",
    "inv_map = {v: k for k, v in class_mapping.items()}\n",
    "\n",
    "print('Training images per class:')\n",
    "pprint.pprint(classes_count)\n",
    "print('Num classes (including bg) = {}'.format(len(classes_count)))\n",
    "\n",
    "config_output_filename =  config_filename\n",
    "\n",
    "with open(modeldir + config_output_filename, 'wb') as config_f:\n",
    "    pickle.dump(C,config_f)\n",
    "    print('Config has been written to {}, and can be loaded when testing to ensure correct results'.format(config_output_filename))\n",
    "\n",
    "random.shuffle(all_imgs)\n",
    "\n",
    "num_imgs = len(all_imgs)\n",
    "\n",
    "train_imgs = [s for s in all_imgs if s['imageset'] == 'trainval']\n",
    "val_imgs = [s for s in all_imgs if s['imageset'] == 'test']\n",
    "\n",
    "print('Num train samples {}'.format(len(train_imgs)))\n",
    "print('Num val samples {}'.format(len(val_imgs)))\n",
    "\n",
    "\n",
    "data_gen_train = data_generators.get_anchor_gt(train_imgs, classes_count, C, nn.get_img_output_length, mode='train')\n",
    "data_gen_val = data_generators.get_anchor_gt(val_imgs, classes_count, C, nn.get_img_output_length, mode='val')\n",
    "\n",
    "num_features = 1024\n",
    "input_shape_img = (None, None, 3)\n",
    "input_shape_features = (None, None, num_features)\n",
    "img_input = Input(shape=input_shape_img)\n",
    "roi_input = Input(shape=(None, 4))\n",
    "\n",
    "# define the base network (resnet here, can be VGG, Inception, etc)\n",
    "shared_layers = nn.nn_base(img_input, trainable=True)\n",
    "\n",
    "# define the RPN, built on the base layers\n",
    "num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\n",
    "rpn = nn.rpn(shared_layers, num_anchors)\n",
    "\n",
    "classifier = nn.classifier(shared_layers, roi_input, C.num_rois, nb_classes=len(classes_count), trainable=True)\n",
    "\n",
    "model_rpn = Model(img_input, rpn[:2])\n",
    "model_classifier = Model([img_input, roi_input], classifier)\n",
    "\n",
    "# this is a model that holds both the RPN and the classifier, used to load/save weights for the models\n",
    "model_all = Model([img_input, roi_input], rpn[:2] + classifier)\n",
    "\n",
    "weights_url = \"https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5\"\n",
    "if C.base_net_weights not in os.listdir(os.getcwd()):\n",
    "    print('Could not find pretrained model weights.')\n",
    "    get_file(C.base_net_weights, weights_url, cache_subdir=os.getcwd())\n",
    "\n",
    "print('loading weights from {}'.format(C.base_net_weights))\n",
    "model_rpn.load_weights(C.base_net_weights, by_name=True)\n",
    "model_classifier.load_weights(C.base_net_weights, by_name=True)\n",
    "    \n",
    "optimizer = Adam(lr=1e-5)\n",
    "optimizer_classifier = Adam(lr=1e-5)\n",
    "model_rpn.compile(optimizer=optimizer, loss=[losses.rpn_loss_cls(num_anchors), losses.rpn_loss_regr(num_anchors)])\n",
    "model_classifier.compile(optimizer=optimizer_classifier, loss=[losses.class_loss_cls, losses.class_loss_regr(len(classes_count)-1)], metrics={'dense_class_{}'.format(len(classes_count)): 'accuracy'})\n",
    "model_all.compile(optimizer='sgd', loss='mae')\n",
    "\n",
    "epoch_length = 10 #len(train_imgs)\n",
    "num_epochs = int( num_epochs)\n",
    "iter_num = 0\n",
    "\n",
    "losses = np.zeros((epoch_length, 5))\n",
    "rpn_accuracy_rpn_monitor = []\n",
    "rpn_accuracy_for_epoch = []\n",
    "start_time = time.time()\n",
    "\n",
    "best_loss = np.Inf\n",
    "\n",
    "class_mapping_inv = {v: k for k, v in class_mapping.items()}\n",
    "print('Starting training')\n",
    "\n",
    "vis = True\n",
    "\n",
    "\n",
    "\n",
    "callback = TensorBoard(log_path)\n",
    "callback.set_model(model_all)\n",
    "train_metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0218 09:51:31.912305 139714767529792 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of overlapping bounding boxes from RPN = 11.5 for 10 previous iterations\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 12.090909090909092\n",
      "Classifier accuracy for bounding boxes from RPN: 0.68125\n",
      "Loss RPN classifier: 7.042860984802246\n",
      "Loss RPN regression: 0.37945995219051837\n",
      "Loss Detector classifier: 1.0714310824871063\n",
      "Loss Detector regression: 0.3999995470046997\n",
      "Elapsed time: 224.16713523864746\n",
      "Epoch 2/5\n",
      "Average number of overlapping bounding boxes from RPN = 27.7 for 10 previous iterations\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 27.4\n",
      "Classifier accuracy for bounding boxes from RPN: 0.55625\n",
      "Loss RPN classifier: 5.014299631118774\n",
      "Loss RPN regression: 0.29773902222514154\n",
      "Loss Detector classifier: 1.03747016787529\n",
      "Loss Detector regression: 0.44110589623451235\n",
      "Elapsed time: 109.92032551765442\n",
      "Epoch 3/5\n",
      "Average number of overlapping bounding boxes from RPN = 22.3 for 10 previous iterations\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 21.583333333333332\n",
      "Classifier accuracy for bounding boxes from RPN: 0.55\n",
      "Loss RPN classifier: 4.328234100341797\n",
      "Loss RPN regression: 0.5896440580487251\n",
      "Loss Detector classifier: 0.9975813627243042\n",
      "Loss Detector regression: 0.43078863322734834\n",
      "Elapsed time: 117.09505486488342\n",
      "Epoch 4/5\n",
      "Average number of overlapping bounding boxes from RPN = 17.0 for 10 previous iterations\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 15.363636363636363\n",
      "Classifier accuracy for bounding boxes from RPN: 0.61875\n",
      "Loss RPN classifier: 2.927758181095123\n",
      "Loss RPN regression: 0.286210092715919\n",
      "Loss Detector classifier: 0.9274119019508362\n",
      "Loss Detector regression: 0.40606641471385957\n",
      "Elapsed time: 112.6941568851471\n",
      "Epoch 5/5\n",
      "Average number of overlapping bounding boxes from RPN = 23.4 for 10 previous iterations\n",
      "Mean number of bounding boxes from RPN overlapping ground truth boxes: 21.5\n",
      "Classifier accuracy for bounding boxes from RPN: 0.609375\n",
      "Loss RPN classifier: 2.9555697679519652\n",
      "Loss RPN regression: 0.36170869022607804\n",
      "Loss Detector classifier: 0.9191734433174134\n",
      "Loss Detector regression: 0.4401800900697708\n",
      "Elapsed time: 109.25789499282837\n"
     ]
    }
   ],
   "source": [
    "for epoch_num in range(num_epochs):\n",
    "    metric_names = ['loss_rpn_cls','loss_rpn_regr', 'loss_class_cls',\n",
    "                    'loss_class_regr', 'class_acc', 'mean_overlapping_bboxes']\n",
    "#     progbar = generic_utils.Progbar(epoch_length)\n",
    "    print('Epoch {}/{}'.format(epoch_num + 1, num_epochs))\n",
    "    train_metrics = []\n",
    "    train_logs = []\n",
    "    while True:\n",
    "        try:\n",
    "\n",
    "            if len(rpn_accuracy_rpn_monitor) == epoch_length and C.verbose:\n",
    "                mean_overlapping_bboxes = float(sum(rpn_accuracy_rpn_monitor))/len(rpn_accuracy_rpn_monitor)\n",
    "                rpn_accuracy_rpn_monitor = []\n",
    "                print('Average number of overlapping bounding boxes from RPN = {} for {} previous iterations'.format(mean_overlapping_bboxes, epoch_length))\n",
    "                if mean_overlapping_bboxes == 0:\n",
    "                    print('RPN is not producing bounding boxes that overlap the ground truth boxes. Check RPN settings or keep training.')\n",
    "\n",
    "            X, Y, img_data = next(data_gen_train)\n",
    "#             print(Y)\n",
    "            loss_rpn = model_rpn.train_on_batch(X, Y)\n",
    "            P_rpn = model_rpn.predict_on_batch(X)\n",
    "\n",
    "            R = roi_helpers.rpn_to_roi(P_rpn[0], P_rpn[1], C, use_regr=True, overlap_thresh=0.7, max_boxes=300)\n",
    "            # note: calc_iou converts from (x1,y1,x2,y2) to (x,y ,w,h) format\n",
    "            X2, Y1, Y2, IouS = roi_helpers.calc_iou(R, img_data, C, class_mapping)\n",
    "\n",
    "            if X2 is None:\n",
    "                rpn_accuracy_rpn_monitor.append(0)\n",
    "                rpn_accuracy_for_epoch.append(0)\n",
    "                continue\n",
    "\n",
    "            neg_samples = np.where(Y1[0, :, -1] == 1)\n",
    "            pos_samples = np.where(Y1[0, :, -1] == 0)\n",
    "\n",
    "            if len(neg_samples) > 0:\n",
    "                neg_samples = neg_samples[0]\n",
    "            else:\n",
    "                neg_samples = []\n",
    "\n",
    "            if len(pos_samples) > 0:\n",
    "                pos_samples = pos_samples[0]\n",
    "            else:\n",
    "                pos_samples = []\n",
    "            \n",
    "            rpn_accuracy_rpn_monitor.append(len(pos_samples))\n",
    "            rpn_accuracy_for_epoch.append((len(pos_samples)))\n",
    "\n",
    "            if C.num_rois > 1:\n",
    "                if len(pos_samples) < C.num_rois//2:\n",
    "                    selected_pos_samples = pos_samples.tolist()\n",
    "                else:\n",
    "                    selected_pos_samples = np.random.choice(pos_samples, C.num_rois//2, replace=False).tolist()\n",
    "                try:\n",
    "                    selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=False).tolist()\n",
    "                except:\n",
    "                    selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples), replace=True).tolist()\n",
    "\n",
    "                sel_samples = selected_pos_samples + selected_neg_samples\n",
    "            else:\n",
    "                # in the extreme case where num_rois = 1, we pick a random pos or neg sample\n",
    "                selected_pos_samples = pos_samples.tolist()\n",
    "                selected_neg_samples = neg_samples.tolist()\n",
    "                if np.random.randint(0, 2):\n",
    "                    sel_samples = random.choice(neg_samples)\n",
    "                else:\n",
    "                    sel_samples = random.choice(pos_samples)\n",
    "\n",
    "            loss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]], [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])\n",
    "\n",
    "            losses[iter_num, 0] = loss_rpn[1]\n",
    "            losses[iter_num, 1] = loss_rpn[2]\n",
    "\n",
    "            losses[iter_num, 2] = loss_class[1]\n",
    "            losses[iter_num, 3] = loss_class[2]\n",
    "            losses[iter_num, 4] = loss_class[3]\n",
    "\n",
    "            iter_num += 1\n",
    "            \n",
    "            if iter_num == epoch_length:\n",
    "                loss_rpn_cls = np.mean(losses[:, 0])\n",
    "                loss_rpn_regr = np.mean(losses[:, 1])\n",
    "                loss_class_cls = np.mean(losses[:, 2])\n",
    "                loss_class_regr = np.mean(losses[:, 3])\n",
    "                class_acc = np.mean(losses[:, 4])\n",
    "                mean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)\n",
    "                \n",
    "                rpn_accuracy_for_epoch = []\n",
    "                logs = [loss_rpn_cls, loss_rpn_regr, loss_class_cls, loss_class_regr,\n",
    "                       class_acc, mean_overlapping_bboxes]\n",
    "                train_logs = logs\n",
    "                write_log(callback, metric_names, logs, epoch_num)\n",
    "                if C.verbose:\n",
    "                    print('Mean number of bounding boxes from RPN overlapping ground truth boxes: {}'.format(mean_overlapping_bboxes))\n",
    "                    print('Classifier accuracy for bounding boxes from RPN: {}'.format(class_acc))\n",
    "                    print('Loss RPN classifier: {}'.format(loss_rpn_cls))\n",
    "                    print('Loss RPN regression: {}'.format(loss_rpn_regr))\n",
    "                    print('Loss Detector classifier: {}'.format(loss_class_cls))\n",
    "                    print('Loss Detector regression: {}'.format(loss_class_regr))\n",
    "                    print('Elapsed time: {}'.format(time.time() - start_time))\n",
    "\n",
    "                curr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr\n",
    "                iter_num = 0\n",
    "                start_time = time.time()\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print('Exception 3: {}'.format(e))\n",
    "            break\n",
    "    train_metrics = train_logs\n",
    "    train_metrics = np.asarray(train_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving metrics\n",
      "metrics {'metrics': [{'class': 'scalar', 'name': 'loss_rpn_cls', 'value': 2.9555697679519652}, {'class': 'scalar', 'name': 'loss_rpn_regr', 'value': 0.36170869022607804}, {'class': 'scalar', 'name': 'loss_class_cls', 'value': 0.9191734433174134}, {'class': 'scalar', 'name': 'loss_class_regr', 'value': 0.4401800900697708}, {'class': 'scalar', 'name': 'class_acc', 'value': 0.609375}, {'class': 'scalar', 'name': 'mean_overlapping_bboxes', 'value': 21.5}]}\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "Loading weights from /home/dkube/work/workspace/model/weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0218 10:03:58.867692 139714767529792 deprecation.py:323] From <ipython-input-13-7761d652c7fa>:63: simple_save (from tensorflow.python.saved_model.simple_save) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.simple_save.\n",
      "W0218 10:03:58.869027 139714767529792 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved, version =  1\n",
      "Training complete, exiting.\n"
     ]
    }
   ],
   "source": [
    "################# Saving Metrics ###########################\n",
    "metrics = []\n",
    "print(\"saving metrics\")\n",
    "metric_names = ['loss_rpn_cls', 'loss_rpn_regr', 'loss_class_cls', 'loss_class_regr',\n",
    "                       'class_acc', 'mean_overlapping_bboxes']\n",
    "if not tf.io.gfile.exists(metric_path):\n",
    "    tf.io.gfile.makedirs(metric_path)\n",
    "for i in range(6):\n",
    "    temp = {}\n",
    "    temp['class'] = 'scalar'\n",
    "    temp['name'] = metric_names[i]\n",
    "    temp['value'] = train_metrics[i]\n",
    "    metrics.append(temp)\n",
    "metrics = {'metrics':metrics}\n",
    "print('metrics', metrics)\n",
    "with open(metric_path + 'metrics.json', 'w') as outfile:\n",
    "    json.dump(metrics, outfile, indent=4)\n",
    "\n",
    "############### Saving Model ###############################\n",
    "model_all.save(modeldir + 'weights.h5')\n",
    "version = 0\n",
    "if not tf.io.gfile.exists(rpn_export_path):\n",
    "    tf.io.gfile.makedirs(rpn_export_path)\n",
    "if not tf.io.gfile.exists(clf_export_path):\n",
    "    tf.io.gfile.makedirs(clf_export_path)\n",
    "\n",
    "############ Reconfiguring model for inference ##############\n",
    "num_features = 1024\n",
    "input_shape_img = (600,600, 3)\n",
    "input_shape_features = (38, 38, num_features)\n",
    "img_input = Input(shape=input_shape_img)\n",
    "roi_input = Input(shape=(32, 4))\n",
    "feature_map_input = Input(shape=input_shape_features)\n",
    "shared_layers = nn.nn_base(img_input, trainable=True)\n",
    "num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\n",
    "rpn_layers = nn.rpn(shared_layers, num_anchors)\n",
    "classifier = nn.classifier(feature_map_input, roi_input, C.num_rois, nb_classes=len(class_mapping), trainable=True)\n",
    "model_rpn = Model(img_input, rpn_layers)\n",
    "model_classifier = Model([feature_map_input, roi_input], classifier)\n",
    "C.model_path = modeldir + 'weights.h5'\n",
    "\n",
    "print('Loading weights from {}'.format(C.model_path))\n",
    "model_rpn.load_weights(C.model_path, by_name=True)\n",
    "model_classifier.load_weights(C.model_path, by_name=True)\n",
    "\n",
    "model_rpn.compile(optimizer='sgd', loss='mse')\n",
    "model_classifier.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "####################### Saving Protobuf ######################\n",
    "\n",
    "saved_models = tf.io.gfile.listdir(rpn_export_path)\n",
    "saved_models = [int(mdir) for mdir in saved_models if '.' not in mdir]\n",
    "if len(saved_models) < 1:\n",
    "    version = 1\n",
    "else:\n",
    "    version = max(saved_models) + 1\n",
    "tf.keras.backend.set_learning_phase(0)  # Ignore dropout at inference\n",
    "with tf.keras.backend.get_session() as sess:\n",
    "    tf.saved_model.simple_save(\n",
    "        sess,\n",
    "        rpn_export_path + str(version),\n",
    "        inputs={'input': model_rpn.input},\n",
    "        outputs={t.name: t for t in model_rpn.output})\n",
    "    tf.saved_model.simple_save(\n",
    "        sess,\n",
    "        clf_export_path + str(version),\n",
    "        inputs={t.name: t for t in model_classifier.inputs},\n",
    "        outputs={t.name: t for t in model_classifier.output})\n",
    "    \n",
    "print(\"Model saved, version = \", version)\n",
    "print('Training complete, exiting.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}